{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451c744e-0124-4491-9ed1-4c7ff0fc0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libaries\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf2402-bfc7-418e-a050-77665cecf023",
   "metadata": {},
   "source": [
    "**Exploring the Local Strom Report and National Flood Insturance Program (NFIP) database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86981b97-4abf-487f-b47e-bb2f20805d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory of the LSR and pluvial claim dataset\n",
    "LSR = Path(\"../data/CONUS/LSR_pluvial_CONUS_X.csv\")\n",
    "pluvial_claims = Path(\"../data/CONUS/Claim_CONUS_2005_X.csv\")\n",
    "\n",
    "output_path = \"../data/CONUS/Claim_CONUS_2005_X_withdamage.csv\"\n",
    "#Out put dir\n",
    "out_dir = \"../data/CONUS/LSR_pluvial_CONUS_X_damagepercent.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3960f92e-abc3-44bd-a9d2-4f76298407a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VALID', 'VALID2', 'LAT', 'LON', 'MAG', 'WFO', 'TYPECODE', 'TYPETEXT',\n",
       "       'CITY', 'COUNTY', 'STATE', 'SOURCE', 'REMARK', 'UGC', 'UGCNAME',\n",
       "       'QUALIFIER', 'OBJECTID', 'Join_Count', 'TARGET_FID', 'STATE_ABBR',\n",
       "       'STATE_FIPS', 'COUNTY_FIPS', 'STCOFIPS', 'TRACT_FIPS', 'FIPS',\n",
       "       'POPULATION', 'POP_SQMI', 'SQMI', 'POPULATION_2020', 'POP20_SQMI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lsr = pd.read_csv(LSR)\n",
    "df_lsr.columns\n",
    "#Note: The FIPS is the census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33bf28c1-8fc1-4719-b708-801717c93a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['agricultureStructureIndicator', 'asOfDate', 'baseFloodElevation',\n",
       "       'basementEnclosureCrawlspace', 'reportedCity', 'condominiumIndicator',\n",
       "       'policyCount', 'countyCode', 'communityRatingSystemDiscount',\n",
       "       'dateOfLoss', 'elevatedBuildingIndicator',\n",
       "       'elevationCertificateIndicator', 'elevationDifference', 'censusTract',\n",
       "       'floodZone', 'houseWorship', 'latitude', 'locationOfContents',\n",
       "       'longitude', 'lowestAdjacentGrade', 'lowestFloorElevation',\n",
       "       'numberOfFloorsInTheInsuredBuilding', 'nonProfitIndicator',\n",
       "       'obstructionType', 'occupancyType', 'originalConstructionDate',\n",
       "       'originalNBDate', 'amountPaidOnBuildingClaim',\n",
       "       'amountPaidOnContentsClaim',\n",
       "       'amountPaidOnIncreasedCostOfComplianceClaim',\n",
       "       'postFIRMConstructionIndicator', 'rateMethod',\n",
       "       'smallBusinessIndicatorBuilding', 'state',\n",
       "       'totalBuildingInsuranceCoverage', 'totalContentsInsuranceCoverage',\n",
       "       'yearOfLoss', 'reportedZipcode', 'primaryResidence', 'id',\n",
       "       'causedBy100yr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pluvialclaims = pd.read_csv(pluvial_claims)\n",
    "df_pluvialclaims.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df8d1c-310c-48b3-b471-28a4fd184023",
   "metadata": {},
   "source": [
    "**Getting the damage (%) from NFIP data and based on the census tract boundary, adding the damage % column into the LSR event database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee64e5c-1a6a-4d78-a22d-5b60f36d8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_damage_percent(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Filling NaNs with 0 for relevant columns\n",
    "    cols = ['amountPaidOnBuildingClaim', 'amountPaidOnContentsClaim',\n",
    "            'totalBuildingInsuranceCoverage', 'totalContentsInsuranceCoverage']\n",
    "    df[cols] = df[cols].fillna(0)\n",
    "\n",
    "    # Initialize damagePercent column\n",
    "    df['damagePercent'] = np.nan\n",
    "\n",
    "    # Both building and contents valid\n",
    "    both_mask = (\n",
    "        (df['totalBuildingInsuranceCoverage'] > 0) &\n",
    "        (df['totalContentsInsuranceCoverage'] > 0) &\n",
    "        (df['amountPaidOnBuildingClaim'] > 0) &\n",
    "        (df['amountPaidOnContentsClaim'] > 0)\n",
    "    )\n",
    "    df.loc[both_mask, 'damagePercent'] = (\n",
    "        df['amountPaidOnBuildingClaim'] + df['amountPaidOnContentsClaim']\n",
    "    ) / (\n",
    "        df['totalBuildingInsuranceCoverage'] + df['totalContentsInsuranceCoverage']\n",
    "    ) * 100\n",
    "\n",
    "    # Only building info\n",
    "    building_mask = (\n",
    "        ((df['totalContentsInsuranceCoverage'] == 0) | (df['amountPaidOnContentsClaim'] == 0)) &\n",
    "        (df['totalBuildingInsuranceCoverage'] > 0) &\n",
    "        (df['amountPaidOnBuildingClaim'] > 0)\n",
    "    )\n",
    "    df.loc[building_mask, 'damagePercent'] = (\n",
    "        df['amountPaidOnBuildingClaim'] / df['totalBuildingInsuranceCoverage']\n",
    "    ) * 100\n",
    "\n",
    "    # Only contents info\n",
    "    contents_mask = (\n",
    "        ((df['totalBuildingInsuranceCoverage'] == 0) | (df['amountPaidOnBuildingClaim'] == 0)) &\n",
    "        (df['totalContentsInsuranceCoverage'] > 0) &\n",
    "        (df['amountPaidOnContentsClaim'] > 0)\n",
    "    )\n",
    "    df.loc[contents_mask, 'damagePercent'] = (\n",
    "        df['amountPaidOnContentsClaim'] / df['totalContentsInsuranceCoverage']\n",
    "    ) * 100\n",
    "\n",
    "    # Cap at 100%\n",
    "    df['damagePercent'] = df['damagePercent'].clip(upper=100)\n",
    "\n",
    "    # Convert dateOfLoss to datetime\n",
    "    df['dateOfLoss'] = pd.to_datetime(df['dateOfLoss'], errors='coerce')\n",
    "\n",
    "    # Compute groupwise mean\n",
    "    group_means = df.groupby(['censusTract', 'dateOfLoss'])['damagePercent'].mean().reset_index(name='meanDamagePercent')\n",
    "\n",
    "    # Take one representative row from each group\n",
    "    first_rows = df.sort_values('dateOfLoss').groupby(['censusTract', 'dateOfLoss']).first().reset_index()\n",
    "\n",
    "    # Merge mean damage percent\n",
    "    result = first_rows.merge(group_means, on=['censusTract', 'dateOfLoss'], how='left')\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facfa5df-1b8d-489c-8fc1-a5f5cb6b245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to: ../data/CONUS/Claim_CONUS_2005_X_withdamage.csv\n"
     ]
    }
   ],
   "source": [
    "# Add damagePercent column\n",
    "df_with_damage = calculate_damage_percent(df_pluvialclaims)\n",
    "\n",
    "# Export to CSV\n",
    "df_with_damage.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Exported to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63407813-86c3-444c-b30a-9b422706715d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>censusTract</th>\n",
       "      <th>agricultureStructureIndicator</th>\n",
       "      <th>baseFloodElevation</th>\n",
       "      <th>basementEnclosureCrawlspace</th>\n",
       "      <th>policyCount</th>\n",
       "      <th>countyCode</th>\n",
       "      <th>communityRatingSystemDiscount</th>\n",
       "      <th>elevatedBuildingIndicator</th>\n",
       "      <th>elevationCertificateIndicator</th>\n",
       "      <th>elevationDifference</th>\n",
       "      <th>...</th>\n",
       "      <th>rateMethod</th>\n",
       "      <th>smallBusinessIndicatorBuilding</th>\n",
       "      <th>totalBuildingInsuranceCoverage</th>\n",
       "      <th>totalContentsInsuranceCoverage</th>\n",
       "      <th>yearOfLoss</th>\n",
       "      <th>reportedZipcode</th>\n",
       "      <th>primaryResidence</th>\n",
       "      <th>causedBy100yr</th>\n",
       "      <th>damagePercent</th>\n",
       "      <th>meanDamagePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.995400e+04</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>30879.000000</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>20089.000000</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>47039.000000</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>4.995400e+04</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>49950.000000</td>\n",
       "      <td>49954.000000</td>\n",
       "      <td>49954.0</td>\n",
       "      <td>36381.000000</td>\n",
       "      <td>36381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.276592e+10</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>253.575000</td>\n",
       "      <td>1.203666</td>\n",
       "      <td>1.045142</td>\n",
       "      <td>32765.624454</td>\n",
       "      <td>6.823635</td>\n",
       "      <td>0.133082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.148148</td>\n",
       "      <td>...</td>\n",
       "      <td>5.700908</td>\n",
       "      <td>0.009909</td>\n",
       "      <td>1.767261e+05</td>\n",
       "      <td>66965.894623</td>\n",
       "      <td>2013.231393</td>\n",
       "      <td>46903.723724</td>\n",
       "      <td>0.778056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.191243</td>\n",
       "      <td>15.245818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.482121e+10</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>451.870228</td>\n",
       "      <td>0.919067</td>\n",
       "      <td>1.978966</td>\n",
       "      <td>14821.193840</td>\n",
       "      <td>1.579030</td>\n",
       "      <td>0.339667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.947444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.470606</td>\n",
       "      <td>0.099051</td>\n",
       "      <td>3.454840e+05</td>\n",
       "      <td>60930.862599</td>\n",
       "      <td>4.638434</td>\n",
       "      <td>25508.057923</td>\n",
       "      <td>0.415558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.639440</td>\n",
       "      <td>20.368479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.001020e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1001.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>1002.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.901995e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19019.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>27330.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.377668</td>\n",
       "      <td>2.598400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.702703e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37027.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.933000e+05</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>44558.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.267607</td>\n",
       "      <td>7.012343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.716502e+10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>47165.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>75073.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.555267</td>\n",
       "      <td>18.874606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.604198e+10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2710.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>56041.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.725000e+07</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>99224.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        censusTract  agricultureStructureIndicator  baseFloodElevation  \\\n",
       "count  4.995400e+04                   49954.000000           80.000000   \n",
       "mean   3.276592e+10                       0.000200          253.575000   \n",
       "std    1.482121e+10                       0.014147          451.870228   \n",
       "min    1.001020e+09                       0.000000           -1.000000   \n",
       "25%    1.901995e+10                       0.000000            3.500000   \n",
       "50%    3.702703e+10                       0.000000           45.500000   \n",
       "75%    4.716502e+10                       0.000000          218.000000   \n",
       "max    5.604198e+10                       1.000000         2710.000000   \n",
       "\n",
       "       basementEnclosureCrawlspace   policyCount    countyCode  \\\n",
       "count                 30879.000000  49954.000000  49954.000000   \n",
       "mean                      1.203666      1.045142  32765.624454   \n",
       "std                       0.919067      1.978966  14821.193840   \n",
       "min                       0.000000      1.000000   1001.000000   \n",
       "25%                       1.000000      1.000000  19019.000000   \n",
       "50%                       1.000000      1.000000  37027.000000   \n",
       "75%                       2.000000      1.000000  47165.000000   \n",
       "max                       4.000000    189.000000  56041.000000   \n",
       "\n",
       "       communityRatingSystemDiscount  elevatedBuildingIndicator  \\\n",
       "count                   20089.000000               49954.000000   \n",
       "mean                        6.823635                   0.133082   \n",
       "std                         1.579030                   0.339667   \n",
       "min                         1.000000                   0.000000   \n",
       "25%                         6.000000                   0.000000   \n",
       "50%                         7.000000                   0.000000   \n",
       "75%                         8.000000                   0.000000   \n",
       "max                        10.000000                   1.000000   \n",
       "\n",
       "       elevationCertificateIndicator  elevationDifference  ...    rateMethod  \\\n",
       "count                            0.0           108.000000  ...  47039.000000   \n",
       "mean                             NaN            14.148148  ...      5.700908   \n",
       "std                              NaN           107.947444  ...      2.470606   \n",
       "min                              NaN           -22.000000  ...      1.000000   \n",
       "25%                              NaN            -1.250000  ...      7.000000   \n",
       "50%                              NaN             0.000000  ...      7.000000   \n",
       "75%                              NaN             1.000000  ...      7.000000   \n",
       "max                              NaN           796.000000  ...      7.000000   \n",
       "\n",
       "       smallBusinessIndicatorBuilding  totalBuildingInsuranceCoverage  \\\n",
       "count                    49954.000000                    4.995400e+04   \n",
       "mean                         0.009909                    1.767261e+05   \n",
       "std                          0.099051                    3.454840e+05   \n",
       "min                          0.000000                    0.000000e+00   \n",
       "25%                          0.000000                    1.000000e+05   \n",
       "50%                          0.000000                    1.933000e+05   \n",
       "75%                          0.000000                    2.500000e+05   \n",
       "max                          1.000000                    4.725000e+07   \n",
       "\n",
       "       totalContentsInsuranceCoverage    yearOfLoss  reportedZipcode  \\\n",
       "count                    49954.000000  49954.000000     49950.000000   \n",
       "mean                     66965.894623   2013.231393     46903.723724   \n",
       "std                      60930.862599      4.638434     25508.057923   \n",
       "min                          0.000000   2005.000000      1002.000000   \n",
       "25%                      30000.000000   2009.000000     27330.000000   \n",
       "50%                      60000.000000   2013.000000     44558.000000   \n",
       "75%                     100000.000000   2017.000000     75073.500000   \n",
       "max                     500000.000000   2021.000000     99224.000000   \n",
       "\n",
       "       primaryResidence  causedBy100yr  damagePercent  meanDamagePercent  \n",
       "count      49954.000000        49954.0   36381.000000       36381.000000  \n",
       "mean           0.778056            0.0      15.191243          15.245818  \n",
       "std            0.415558            0.0      21.639440          20.368479  \n",
       "min            0.000000            0.0       0.000414           0.000414  \n",
       "25%            1.000000            0.0       2.377668           2.598400  \n",
       "50%            1.000000            0.0       6.267607           7.012343  \n",
       "75%            1.000000            0.0      17.555267          18.874606  \n",
       "max            1.000000            0.0     100.000000         100.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Updated dataset\n",
    "df_nfip_with_damage = pd.read_csv(output_path)\n",
    "df_nfip_with_damage.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13c56b6-ea30-40b4-8c77-3f11b898dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned censusTract column and replaced CSV.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(output_path)\n",
    "# Remove decimal and trailing zeros by converting to int then to string\n",
    "df['censusTract'] = df['censusTract'].astype(float).astype('Int64').astype(str)\n",
    "\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Cleaned censusTract column and replaced CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d32089-7aae-4ecd-8d9c-8d1a867c29ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the counts of matching events\n",
    "def getcountsofmatcheddata_dask(nfip_path, lsr_path):\n",
    "    dtypes = {\n",
    "        \"QUALIFIER\": \"object\",\n",
    "        \"FIPS\": \"object\",               \n",
    "        \"VALID\": \"object\",             \n",
    "    }\n",
    "\n",
    "    # Load with Dask using correct dtypes\n",
    "    df_nfip = dd.read_csv(nfip_path, assume_missing=True)\n",
    "    df_lsr = dd.read_csv(lsr_path, dtype=dtypes, assume_missing=True)\n",
    "\n",
    "    # Preprocessing: clean and pad FIPS/censusTract\n",
    "    df_nfip = df_nfip[df_nfip['censusTract'].notnull()]\n",
    "    df_nfip['censusTract'] = df_nfip['censusTract'].astype(float).astype('Int64').astype(str).str.zfill(11)\n",
    "    df_lsr['FIPS'] = df_lsr['FIPS'].astype(str).str.zfill(11)\n",
    "\n",
    "    # Convert date columns\n",
    "    df_nfip['dateOnly'] = dd.to_datetime(df_nfip['dateOfLoss'], errors='coerce').dt.normalize()\n",
    "    df_lsr['LSR_dateOnly'] = dd.to_datetime(\n",
    "        df_lsr['VALID'].astype(str).str[:8], format=\"%Y%m%d\", errors='coerce'\n",
    "    ).dt.normalize()\n",
    "\n",
    "    # Filter valid NFIP rows\n",
    "    df_nfip = df_nfip[(df_nfip['damagePercent'].notnull()) & (df_nfip['dateOnly'].notnull())]\n",
    "\n",
    "    # Bring Dask to Pandas for optimized broadcasting\n",
    "    df_nfip = df_nfip[['censusTract', 'dateOnly']].compute()\n",
    "    df_lsr = df_lsr[['FIPS', 'LSR_dateOnly']].compute()\n",
    "\n",
    "    print(f\"NFIP valid rows: {len(df_nfip)}\")\n",
    "    print(f\"LSR total rows: {len(df_lsr)}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Group LSR by FIPS for fast lookups\n",
    "    lsr_grouped = df_lsr.groupby('FIPS')\n",
    "\n",
    "    for delta in [0, 1, 2, 5, 10, 15, 20]:\n",
    "        matched_count = 0\n",
    "\n",
    "        # Iterate over chunks (or parallelize later)\n",
    "        for _, row in df_nfip.iterrows():\n",
    "            fips = row['censusTract']\n",
    "            date = row['dateOnly']\n",
    "\n",
    "            if fips not in lsr_grouped.groups:\n",
    "                continue\n",
    "\n",
    "            lsr_sub = lsr_grouped.get_group(fips)\n",
    "\n",
    "            if delta == 0:\n",
    "                matched = lsr_sub[lsr_sub['LSR_dateOnly'] == date]\n",
    "            else:\n",
    "                matched = lsr_sub[\n",
    "                    (lsr_sub['LSR_dateOnly'] >= date - pd.Timedelta(days=delta)) &\n",
    "                    (lsr_sub['LSR_dateOnly'] <= date + pd.Timedelta(days=delta))\n",
    "                ]\n",
    "\n",
    "            matched_count += len(matched)\n",
    "\n",
    "        print(f\"±{delta:>2} days → Matched LSR records: {matched_count}\")\n",
    "        results.append((delta, matched_count))\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"±Days\", \"Matched_LSR_Records\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eaf9bd7-72d9-442b-a9e8-d2b164ea3013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFIP valid rows: 36381\n",
      "LSR total rows: 237063\n",
      "± 0 days → Matched LSR records: 2370\n",
      "± 1 days → Matched LSR records: 4144\n",
      "± 2 days → Matched LSR records: 4446\n",
      "± 5 days → Matched LSR records: 4858\n",
      "±10 days → Matched LSR records: 5411\n",
      "±15 days → Matched LSR records: 5834\n",
      "±20 days → Matched LSR records: 6202\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>±Days</th>\n",
       "      <th>Matched_LSR_Records</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>5411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>5834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>6202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ±Days  Matched_LSR_Records\n",
       "0      0                 2370\n",
       "1      1                 4144\n",
       "2      2                 4446\n",
       "3      5                 4858\n",
       "4     10                 5411\n",
       "5     15                 5834\n",
       "6     20                 6202"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcountsofmatcheddata_dask(output_path, LSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48972534-94fa-4aa3-b6ef-4c82966f6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_damagepercent(nfip_path, lsr_path, output_path, day_range=1):\n",
    "    lsr_dtypes = {\n",
    "        'FIPS': 'object',\n",
    "        'VALID': 'object',\n",
    "        'QUALIFIER': 'object' \n",
    "    }\n",
    "\n",
    "    # Load datasets with Dask\n",
    "    df_nfip = dd.read_csv(nfip_path, assume_missing=True)\n",
    "    df_lsr = dd.read_csv(lsr_path, dtype=lsr_dtypes, assume_missing=True)\n",
    "\n",
    "    # Filter and clean NFIP\n",
    "    df_nfip = df_nfip[df_nfip['censusTract'].notnull()]\n",
    "    df_nfip['censusTract'] = df_nfip['censusTract'].astype(float).astype('Int64').astype(str).str.zfill(11)\n",
    "    df_nfip['dateOnly'] = dd.to_datetime(df_nfip['dateOfLoss'], errors='coerce').dt.normalize()\n",
    "    df_nfip = df_nfip[df_nfip['damagePercent'].notnull() & df_nfip['dateOnly'].notnull()]\n",
    "    df_nfip = df_nfip[['censusTract', 'dateOnly', 'damagePercent']].compute()\n",
    "\n",
    "    # Clean LSR\n",
    "    df_lsr['FIPS'] = df_lsr['FIPS'].astype(str).str.zfill(11)\n",
    "    df_lsr['LSR_dateOnly'] = dd.to_datetime(\n",
    "        df_lsr['VALID'].astype(str).str[:8], format=\"%Y%m%d\", errors='coerce'\n",
    "    ).dt.normalize()\n",
    "\n",
    "    df_lsr = df_lsr.compute()\n",
    "    df_lsr['damagePercent'] = pd.NA\n",
    "\n",
    "    print(f\"NFIP valid rows: {len(df_nfip)}\")\n",
    "    print(f\"LSR total rows: {len(df_lsr)}\")\n",
    "\n",
    "    # Expand NFIP by ±day_range\n",
    "    if day_range == 0:\n",
    "        nfip_expanded = df_nfip.copy()\n",
    "    else:\n",
    "        offsets = range(-day_range, day_range + 1)\n",
    "        nfip_expanded = pd.DataFrame([\n",
    "            {'censusTract': row['censusTract'],\n",
    "             'dateOnly': row['dateOnly'] + pd.Timedelta(days=offset),\n",
    "             'damagePercent': row['damagePercent']}\n",
    "            for _, row in df_nfip.iterrows()\n",
    "            for offset in offsets\n",
    "        ])\n",
    "\n",
    "    # Merge LSR with expanded NFIP by FIPS and Date\n",
    "    merged = pd.merge(\n",
    "        df_lsr,\n",
    "        nfip_expanded,\n",
    "        left_on=['FIPS', 'LSR_dateOnly'],\n",
    "        right_on=['censusTract', 'dateOnly'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    merged['damagePercent'] = merged['damagePercent_x'].combine_first(merged['damagePercent_y'])\n",
    "    merged.drop(columns=['censusTract', 'dateOnly', 'damagePercent_x', 'damagePercent_y', 'LSR_dateOnly'], inplace=True)\n",
    "\n",
    "    # Save output\n",
    "    merged.to_csv(output_path, index=False)\n",
    "    print(f\"Exported LSR with damagePercent using ±{day_range} day range to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac048f41-971b-425f-abe1-c3aa823057f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFIP valid rows: 36381\n",
      "LSR total rows: 237063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1808/986435165.py:53: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged['damagePercent'] = merged['damagePercent_x'].combine_first(merged['damagePercent_y'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported LSR with damagePercent using ±1 day range to:\n",
      "../data/CONUS/LSR_pluvial_CONUS_X_damagepercent.csv\n"
     ]
    }
   ],
   "source": [
    "#Get the geodataframe having the damage percentage\n",
    "transfer_damagepercent(output_path, LSR, out_dir, day_range=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f7d837-e617-4171-a73f-e71871af7ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1808/702973804.py:1: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_outdir = pd.read_csv(out_dir)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALID</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>MAG</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Join_Count</th>\n",
       "      <th>TARGET_FID</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>COUNTY_FIPS</th>\n",
       "      <th>STCOFIPS</th>\n",
       "      <th>TRACT_FIPS</th>\n",
       "      <th>POPULATION</th>\n",
       "      <th>POP_SQMI</th>\n",
       "      <th>SQMI</th>\n",
       "      <th>POPULATION_2020</th>\n",
       "      <th>POP20_SQMI</th>\n",
       "      <th>damagePercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.376010e+05</td>\n",
       "      <td>237601.000000</td>\n",
       "      <td>237601.000000</td>\n",
       "      <td>171017.000000</td>\n",
       "      <td>237601.000000</td>\n",
       "      <td>237601.000000</td>\n",
       "      <td>237601.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>237597.000000</td>\n",
       "      <td>4144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.016372e+11</td>\n",
       "      <td>39.766937</td>\n",
       "      <td>-91.661802</td>\n",
       "      <td>2.199409</td>\n",
       "      <td>118522.766996</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>118522.766996</td>\n",
       "      <td>27.916379</td>\n",
       "      <td>88.451782</td>\n",
       "      <td>28004.831197</td>\n",
       "      <td>381795.103486</td>\n",
       "      <td>3949.952339</td>\n",
       "      <td>1105.976251</td>\n",
       "      <td>147.014353</td>\n",
       "      <td>3925.802569</td>\n",
       "      <td>1096.300645</td>\n",
       "      <td>20.780139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.626019e+08</td>\n",
       "      <td>4.086102</td>\n",
       "      <td>11.801540</td>\n",
       "      <td>3.368137</td>\n",
       "      <td>68451.295580</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>68451.295580</td>\n",
       "      <td>13.852570</td>\n",
       "      <td>84.117023</td>\n",
       "      <td>13859.650815</td>\n",
       "      <td>428509.126537</td>\n",
       "      <td>1663.496982</td>\n",
       "      <td>2818.022385</td>\n",
       "      <td>443.382590</td>\n",
       "      <td>1596.832195</td>\n",
       "      <td>2796.650212</td>\n",
       "      <td>25.921894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.005010e+11</td>\n",
       "      <td>25.480000</td>\n",
       "      <td>-124.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1001.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.013071e+11</td>\n",
       "      <td>37.600000</td>\n",
       "      <td>-96.210000</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>59183.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>59183.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>18049.000000</td>\n",
       "      <td>10300.000000</td>\n",
       "      <td>2790.000000</td>\n",
       "      <td>25.800000</td>\n",
       "      <td>3.590000</td>\n",
       "      <td>2798.000000</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>3.805160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.017072e+11</td>\n",
       "      <td>40.280000</td>\n",
       "      <td>-89.640000</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>118519.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>118519.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>26103.000000</td>\n",
       "      <td>80100.000000</td>\n",
       "      <td>3743.000000</td>\n",
       "      <td>132.100000</td>\n",
       "      <td>30.140000</td>\n",
       "      <td>3743.000000</td>\n",
       "      <td>131.700000</td>\n",
       "      <td>9.213192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.020072e+11</td>\n",
       "      <td>42.550000</td>\n",
       "      <td>-84.630000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>177813.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>177813.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>39017.000000</td>\n",
       "      <td>950400.000000</td>\n",
       "      <td>4902.000000</td>\n",
       "      <td>1190.700000</td>\n",
       "      <td>131.590000</td>\n",
       "      <td>4872.000000</td>\n",
       "      <td>1180.200000</td>\n",
       "      <td>28.353152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.024123e+11</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>-66.990000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>237063.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>237063.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>56043.000000</td>\n",
       "      <td>989300.000000</td>\n",
       "      <td>37577.000000</td>\n",
       "      <td>210233.300000</td>\n",
       "      <td>8989.530000</td>\n",
       "      <td>37892.000000</td>\n",
       "      <td>213333.300000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              VALID            LAT            LON            MAG  \\\n",
       "count  2.376010e+05  237601.000000  237601.000000  171017.000000   \n",
       "mean   2.016372e+11      39.766937     -91.661802       2.199409   \n",
       "std    4.626019e+08       4.086102      11.801540       3.368137   \n",
       "min    2.005010e+11      25.480000    -124.500000       0.000000   \n",
       "25%    2.013071e+11      37.600000     -96.210000       1.180000   \n",
       "50%    2.017072e+11      40.280000     -89.640000       1.780000   \n",
       "75%    2.020072e+11      42.550000     -84.630000       2.750000   \n",
       "max    2.024123e+11      49.000000     -66.990000    1017.000000   \n",
       "\n",
       "            OBJECTID     Join_Count     TARGET_FID     STATE_FIPS  \\\n",
       "count  237601.000000  237601.000000  237601.000000  237597.000000   \n",
       "mean   118522.766996       0.999983  118522.766996      27.916379   \n",
       "std     68451.295580       0.004103   68451.295580      13.852570   \n",
       "min         1.000000       0.000000       1.000000       1.000000   \n",
       "25%     59183.000000       1.000000   59183.000000      18.000000   \n",
       "50%    118519.000000       1.000000  118519.000000      26.000000   \n",
       "75%    177813.000000       1.000000  177813.000000      39.000000   \n",
       "max    237063.000000       1.000000  237063.000000      56.000000   \n",
       "\n",
       "         COUNTY_FIPS       STCOFIPS     TRACT_FIPS     POPULATION  \\\n",
       "count  237597.000000  237597.000000  237597.000000  237597.000000   \n",
       "mean       88.451782   28004.831197  381795.103486    3949.952339   \n",
       "std        84.117023   13859.650815  428509.126537    1663.496982   \n",
       "min         1.000000    1001.000000     100.000000       0.000000   \n",
       "25%        31.000000   18049.000000   10300.000000    2790.000000   \n",
       "50%        73.000000   26103.000000   80100.000000    3743.000000   \n",
       "75%       123.000000   39017.000000  950400.000000    4902.000000   \n",
       "max       840.000000   56043.000000  989300.000000   37577.000000   \n",
       "\n",
       "            POP_SQMI           SQMI  POPULATION_2020     POP20_SQMI  \\\n",
       "count  237597.000000  237597.000000    237597.000000  237597.000000   \n",
       "mean     1105.976251     147.014353      3925.802569    1096.300645   \n",
       "std      2818.022385     443.382590      1596.832195    2796.650212   \n",
       "min         0.000000       0.020000         0.000000       0.000000   \n",
       "25%        25.800000       3.590000      2798.000000      26.100000   \n",
       "50%       132.100000      30.140000      3743.000000     131.700000   \n",
       "75%      1190.700000     131.590000      4872.000000    1180.200000   \n",
       "max    210233.300000    8989.530000     37892.000000  213333.300000   \n",
       "\n",
       "       damagePercent  \n",
       "count    4144.000000  \n",
       "mean       20.780139  \n",
       "std        25.921894  \n",
       "min         0.058180  \n",
       "25%         3.805160  \n",
       "50%         9.213192  \n",
       "75%        28.353152  \n",
       "max       100.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outdir = pd.read_csv(out_dir)\n",
    "df_outdir.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77835bc-87c3-4164-8902-b03a9f23c02b",
   "metadata": {},
   "source": [
    "**Get the selected LSR incidents only for 1 day window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cfe5677-1782-4c90-bc66-8e9aec399e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_damagepercent_matched_only(nfip_path, lsr_path, output_path, day_range=1):\n",
    "    import dask.dataframe as dd\n",
    "    import pandas as pd\n",
    "\n",
    "    lsr_dtypes = {\n",
    "        'FIPS': 'object',\n",
    "        'VALID': 'object',\n",
    "        'QUALIFIER': 'object' \n",
    "    }\n",
    "\n",
    "    # Load datasets with Dask\n",
    "    df_nfip = dd.read_csv(nfip_path, assume_missing=True)\n",
    "    df_lsr = dd.read_csv(lsr_path, dtype=lsr_dtypes, assume_missing=True)\n",
    "\n",
    "    # Clean and filter NFIP\n",
    "    df_nfip = df_nfip[df_nfip['censusTract'].notnull()]\n",
    "    df_nfip['censusTract'] = df_nfip['censusTract'].astype(float).astype('Int64').astype(str).str.zfill(11)\n",
    "    df_nfip['dateOnly'] = dd.to_datetime(df_nfip['dateOfLoss'], errors='coerce').dt.normalize()\n",
    "    df_nfip = df_nfip[df_nfip['damagePercent'].notnull() & df_nfip['dateOnly'].notnull()]\n",
    "    df_nfip = df_nfip[['censusTract', 'dateOnly', 'damagePercent']].compute()\n",
    "\n",
    "    # Clean LSR\n",
    "    df_lsr['FIPS'] = df_lsr['FIPS'].astype(str).str.zfill(11)\n",
    "    df_lsr['LSR_dateOnly'] = dd.to_datetime(\n",
    "        df_lsr['VALID'].astype(str).str[:8], format=\"%Y%m%d\", errors='coerce'\n",
    "    ).dt.normalize()\n",
    "    df_lsr = df_lsr.compute()\n",
    "    df_lsr['damagePercent'] = pd.NA\n",
    "\n",
    "    print(f\"NFIP valid rows: {len(df_nfip)}\")\n",
    "    print(f\"LSR total rows: {len(df_lsr)}\")\n",
    "\n",
    "    # Expand NFIP by ±day_range\n",
    "    if day_range == 0:\n",
    "        nfip_expanded = df_nfip.copy()\n",
    "    else:\n",
    "        offsets = range(-day_range, day_range + 1)\n",
    "        nfip_expanded = pd.DataFrame([\n",
    "            {'censusTract': row['censusTract'],\n",
    "             'dateOnly': row['dateOnly'] + pd.Timedelta(days=offset),\n",
    "             'damagePercent': row['damagePercent']}\n",
    "            for _, row in df_nfip.iterrows()\n",
    "            for offset in offsets\n",
    "        ])\n",
    "\n",
    "    # Merge LSR with expanded NFIP\n",
    "    merged = pd.merge(\n",
    "        df_lsr,\n",
    "        nfip_expanded,\n",
    "        left_on=['FIPS', 'LSR_dateOnly'],\n",
    "        right_on=['censusTract', 'dateOnly'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Keep only matched rows\n",
    "    matched = merged[merged['damagePercent_y'].notnull()].copy()\n",
    "\n",
    "    # Use matched damage percent\n",
    "    matched['damagePercent'] = matched['damagePercent_y']\n",
    "    matched.drop(columns=['censusTract', 'dateOnly', 'damagePercent_x', 'damagePercent_y', 'LSR_dateOnly'], inplace=True)\n",
    "\n",
    "    # Save only matched rows\n",
    "    matched.to_csv(output_path, index=False)\n",
    "    print(f\"Exported ONLY MATCHED LSR rows (±{day_range} day window) to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "029ab8a4-425f-47e7-9ccb-dbe0a8da1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFIP valid rows: 36381\n",
      "LSR total rows: 237063\n",
      "Exported ONLY MATCHED LSR rows (±1 day window) to:\n",
      "../data/CONUS/selected_LSR_conus_for1daywindow.csv\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"../data/CONUS/selected_LSR_conus_for1daywindow.csv\")\n",
    "transfer_damagepercent_matched_only(output_path, LSR, save_dir, day_range=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c1262-dfa7-406e-83bd-278773713aac",
   "metadata": {},
   "source": [
    "**Assigning the NextGen catchment ID to all the LSR incidents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8373e43-826d-4195-9601-bcbc23972094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nextgen directory\n",
    "ngen_dir = Path(\"../data/conus_nextgenHF.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8ed0aac-ff17-44ca-9beb-809de8a3f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning catchments for all LSR incidents\n",
    "def ngen_cat(csv_path, geopkg_path, output_path, layer_name=\"divides\", \n",
    "                               lat_col=\"LAT\", lon_col=\"LON\", catchment_col=\"divide_id\"):\n",
    "    \"\"\"\n",
    "    Assigns nextgen catchments to points in a CSV based on a polygon layer in a GeoPackage.\n",
    "\n",
    "    Parameters:\n",
    "        csv_path (str or Path): Path to the input CSV with latitude and longitude columns.\n",
    "        geopkg_path (str or Path): Path to the GeoPackage file.\n",
    "        layer_name (str): Name of the polygon layer (default: \"divides\").\n",
    "        lat_col (str): Name of the latitude column in CSV (default: \"latitude\").\n",
    "        lon_col (str): Name of the longitude column in CSV (default: \"longitude\").\n",
    "        catchment_col (str): Name of the catchment ID or name column in the divides layer (default: \"name\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original CSV with an added 'nextgen_catchment' column.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    geometry = [Point(xy) for xy in zip(df[lon_col], df[lat_col])]\n",
    "    gdf_points = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Load divides layer\n",
    "    gdf_divides = gpd.read_file(geopkg_path, layer=layer_name)\n",
    "\n",
    "    # Match CRS\n",
    "    if gdf_points.crs != gdf_divides.crs:\n",
    "        gdf_points = gdf_points.to_crs(gdf_divides.crs)\n",
    "        \n",
    "    joined = gpd.sjoin(gdf_points, gdf_divides[[catchment_col, \"geometry\"]], \n",
    "                       how=\"left\", predicate=\"within\")\n",
    "    joined_clean = joined[~joined.index.duplicated(keep=\"first\")]\n",
    "    df['nextgen_catchment'] = joined_clean[catchment_col].reindex(df.index)\n",
    "\n",
    "    # Save to file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved with nextgen_catchment to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c86d7a8b-ad89-4500-9851-9bb1e819ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with nextgen_catchment to: ../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat.csv\n"
     ]
    }
   ],
   "source": [
    "ngen_cat(csv_path=save_dir, geopkg_path = ngen_dir, output_path = \"../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb0a5da-ccaf-4cb2-9e7a-bcd943c8c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid with the same day multiple LSR information for a single NextGen Catchment\n",
    "def filter_events(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Parse datetime column\n",
    "    df['VALID2'] = pd.to_datetime(df['VALID2'], errors='coerce')\n",
    "\n",
    "    # Create date-only column \n",
    "    df['event_date'] = df['VALID2'].dt.date\n",
    "\n",
    "    def select_event(group):\n",
    "        event_types = set(group['TYPETEXT'].str.upper())\n",
    "        if 'FLASH FLOOD' in event_types:\n",
    "            filtered = group[group['TYPETEXT'].str.upper() == 'FLASH FLOOD']\n",
    "        elif 'HEAVY RAIN' in event_types:\n",
    "            filtered = group[group['TYPETEXT'].str.upper() == 'HEAVY RAIN']\n",
    "        else:\n",
    "            return None  # if neither event type is present\n",
    "        return filtered.sort_values('VALID2').head(1)\n",
    "\n",
    "    filtered_df = (\n",
    "        df.groupby(['nextgen_catchment', 'event_date'], group_keys=False)\n",
    "        .apply(select_event)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved filtered results to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "892e0c3c-0b1f-49fc-a12c-224e2a6ef964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered results to ../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1808/1509929466.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(select_event)\n"
     ]
    }
   ],
   "source": [
    "filter_events(\"../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat.csv\", \"../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c6841-b57c-408b-95df-6efe07f3238d",
   "metadata": {},
   "source": [
    "**Exploring the dataset about having multiple incidents in the same catchment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0fb1ce-9b47-4e2f-b817-3b8fb68c6ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique catchments with at least one damage record: 2321\n",
      "Number of catchments with more than 2 distinct damage records: 255\n",
      "Total number of events from catchments with >1 unique damagePercent values: 595\n",
      "Catchment IDs:\n",
      "['cat-100224', 'cat-1010263', 'cat-1021677', 'cat-1023764', 'cat-102612', 'cat-103066', 'cat-105032', 'cat-105049', 'cat-105101', 'cat-105663', 'cat-105836', 'cat-1077437', 'cat-1077455', 'cat-1077921', 'cat-1078797', 'cat-1078883', 'cat-1078884', 'cat-1078889', 'cat-1078901', 'cat-1078916', 'cat-1079044', 'cat-1079156', 'cat-1079314', 'cat-1079621', 'cat-1080803', 'cat-1081184', 'cat-1081895', 'cat-1088778', 'cat-1090024', 'cat-1091210', 'cat-1092307', 'cat-1092590', 'cat-1093260', 'cat-1093967', 'cat-1094076', 'cat-1094439', 'cat-1097193', 'cat-109748', 'cat-1098522', 'cat-1098532', 'cat-1100007', 'cat-1100010', 'cat-1100111', 'cat-1100508', 'cat-1100659', 'cat-1100668', 'cat-1100834', 'cat-1103277', 'cat-1106175', 'cat-1106551', 'cat-1111774', 'cat-1111984', 'cat-1121208', 'cat-1121664', 'cat-1122758', 'cat-1123000', 'cat-1123314', 'cat-1123351', 'cat-1123606', 'cat-1123628', 'cat-1125753', 'cat-114960', 'cat-115994', 'cat-116003', 'cat-116210', 'cat-116485', 'cat-116510', 'cat-116555', 'cat-116556', 'cat-116705', 'cat-117089', 'cat-119180', 'cat-121130', 'cat-1315131', 'cat-1324300', 'cat-1324836', 'cat-1328476', 'cat-1334564', 'cat-1338147', 'cat-1338908', 'cat-1339142', 'cat-1339618', 'cat-1539825', 'cat-1540398', 'cat-1550076', 'cat-1555666', 'cat-1559541', 'cat-1561642', 'cat-1562395', 'cat-1564208', 'cat-1570551', 'cat-1580153', 'cat-1582822', 'cat-1585863', 'cat-1590404', 'cat-17457', 'cat-17490', 'cat-1873568', 'cat-2148498', 'cat-2149779', 'cat-2149905', 'cat-2163782', 'cat-2171432', 'cat-2172259', 'cat-2179964', 'cat-2179983', 'cat-2179985', 'cat-2179991', 'cat-2180666', 'cat-2180770', 'cat-2185390', 'cat-2186888', 'cat-2188294', 'cat-2189046', 'cat-2189444', 'cat-2189733', 'cat-2190019', 'cat-2198828', 'cat-2199507', 'cat-2407979', 'cat-2407990', 'cat-2408311', 'cat-2411262', 'cat-2412684', 'cat-2415300', 'cat-2419044', 'cat-2419963', 'cat-2434636', 'cat-2438150', 'cat-2439150', 'cat-2439459', 'cat-2441034', 'cat-2441171', 'cat-2441965', 'cat-247118', 'cat-247359', 'cat-247372', 'cat-247955', 'cat-249028', 'cat-251946', 'cat-2526982', 'cat-256379', 'cat-257900', 'cat-258209', 'cat-258217', 'cat-259429', 'cat-262861', 'cat-263739', 'cat-265724', 'cat-267179', 'cat-270662', 'cat-270916', 'cat-271006', 'cat-271398', 'cat-2716276', 'cat-2720627', 'cat-2748412', 'cat-2751904', 'cat-2877827', 'cat-3052009', 'cat-3056235', 'cat-420580', 'cat-420586', 'cat-420618', 'cat-482252', 'cat-482303', 'cat-487858', 'cat-491530', 'cat-491544', 'cat-491560', 'cat-491602', 'cat-502014', 'cat-505420', 'cat-505611', 'cat-506174', 'cat-506332', 'cat-506560', 'cat-655183', 'cat-658797', 'cat-659075', 'cat-661279', 'cat-787253', 'cat-788423', 'cat-788433', 'cat-789787', 'cat-789788', 'cat-791355', 'cat-791519', 'cat-792075', 'cat-792341', 'cat-794072', 'cat-794236', 'cat-795702', 'cat-796025', 'cat-798139', 'cat-804994', 'cat-805079', 'cat-805158', 'cat-805223', 'cat-805795', 'cat-805799', 'cat-805806', 'cat-807627', 'cat-807656', 'cat-807661', 'cat-809769', 'cat-811597', 'cat-811610', 'cat-811620', 'cat-811621', 'cat-813331', 'cat-813388', 'cat-813434', 'cat-813886', 'cat-813937', 'cat-814543', 'cat-814754', 'cat-814794', 'cat-817013', 'cat-821251', 'cat-822473', 'cat-823812', 'cat-825925', 'cat-826305', 'cat-826367', 'cat-826868', 'cat-827255', 'cat-827260', 'cat-829282', 'cat-830761', 'cat-831306', 'cat-831331', 'cat-832646', 'cat-832650', 'cat-832651', 'cat-832677', 'cat-836391', 'cat-836437', 'cat-837083', 'cat-91390', 'cat-91769', 'cat-91771', 'cat-92138', 'cat-93002', 'cat-93071', 'cat-94417', 'cat-94429', 'cat-94440', 'cat-94536', 'cat-94772', 'cat-95082', 'cat-95097', 'cat-95120', 'cat-96522', 'cat-99536']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat_filtered.csv\")\n",
    "\n",
    "# Counting unique damagePercent per catchment\n",
    "damage_counts = (\n",
    "    df.groupby(\"nextgen_catchment\")[\"damagePercent\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"unique_damage_incidents\")\n",
    ")\n",
    "\n",
    "# Filter to catchments with more than 5 unique damage records\n",
    "catchments_gt5 = damage_counts[damage_counts[\"unique_damage_incidents\"] > 1]\n",
    "\n",
    "# Number of such catchments\n",
    "num_catchments_gt5 = catchments_gt5.shape[0]\n",
    "\n",
    "num_unique_catchments = df[\"nextgen_catchment\"].nunique()\n",
    "print(f\"Total unique catchments with at least one damage record: {num_unique_catchments}\")\n",
    "\n",
    "\n",
    "print(f\"Number of catchments with more than 2 distinct damage records: {num_catchments_gt5}\")\n",
    "matching_events_df = df[df[\"nextgen_catchment\"].isin(catchments_gt5[\"nextgen_catchment\"])]\n",
    "print(f\"Total number of events from catchments with >1 unique damagePercent values: {matching_events_df.shape[0]}\")\n",
    "\n",
    "print(\"Catchment IDs:\")\n",
    "print(catchments_gt5[\"nextgen_catchment\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ed97f3f-01df-4521-bec2-15431138648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/CONUS/selected_LSR_conus_for1daywindow_NgenCat_filtered.csv\")\n",
    "\n",
    "# Count unique damagePercent values per catchment\n",
    "damage_counts = (\n",
    "    df.groupby(\"nextgen_catchment\")[\"damagePercent\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"unique_damage_incidents\")\n",
    ")\n",
    "\n",
    "# Keep only catchments with more than 2 unique damage records\n",
    "catchments_gt2 = damage_counts[damage_counts[\"unique_damage_incidents\"] > 1]\n",
    "\n",
    "# Filter original dataframe to include only those catchments\n",
    "filtered_df = df[df[\"nextgen_catchment\"].isin(catchments_gt2[\"nextgen_catchment\"])]\n",
    "\n",
    "# Save the filtered data to CSV\n",
    "filtered_df.to_csv(\"../data/CONUS/Filtered_LSR_withNexgenCatchmentsgt1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82d732fc-8058-44df-bb77-b27c331ede87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the unique catchments\n",
    "def get_unique_catchments(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    unique_catchments = df['nextgen_catchment'].dropna().unique()\n",
    "    unique_catchments = sorted(unique_catchments)  # optional sorting\n",
    "    count = len(unique_catchments)\n",
    "    \n",
    "    print(f\"Total unique catchments: {count}\")\n",
    "    return unique_catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c529948c-30c8-4c7d-84b9-91991afc8f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique catchments: 255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cat-100224',\n",
       " 'cat-1010263',\n",
       " 'cat-1021677',\n",
       " 'cat-1023764',\n",
       " 'cat-102612',\n",
       " 'cat-103066',\n",
       " 'cat-105032',\n",
       " 'cat-105049',\n",
       " 'cat-105101',\n",
       " 'cat-105663',\n",
       " 'cat-105836',\n",
       " 'cat-1077437',\n",
       " 'cat-1077455',\n",
       " 'cat-1077921',\n",
       " 'cat-1078797',\n",
       " 'cat-1078883',\n",
       " 'cat-1078884',\n",
       " 'cat-1078889',\n",
       " 'cat-1078901',\n",
       " 'cat-1078916',\n",
       " 'cat-1079044',\n",
       " 'cat-1079156',\n",
       " 'cat-1079314',\n",
       " 'cat-1079621',\n",
       " 'cat-1080803',\n",
       " 'cat-1081184',\n",
       " 'cat-1081895',\n",
       " 'cat-1088778',\n",
       " 'cat-1090024',\n",
       " 'cat-1091210',\n",
       " 'cat-1092307',\n",
       " 'cat-1092590',\n",
       " 'cat-1093260',\n",
       " 'cat-1093967',\n",
       " 'cat-1094076',\n",
       " 'cat-1094439',\n",
       " 'cat-1097193',\n",
       " 'cat-109748',\n",
       " 'cat-1098522',\n",
       " 'cat-1098532',\n",
       " 'cat-1100007',\n",
       " 'cat-1100010',\n",
       " 'cat-1100111',\n",
       " 'cat-1100508',\n",
       " 'cat-1100659',\n",
       " 'cat-1100668',\n",
       " 'cat-1100834',\n",
       " 'cat-1103277',\n",
       " 'cat-1106175',\n",
       " 'cat-1106551',\n",
       " 'cat-1111774',\n",
       " 'cat-1111984',\n",
       " 'cat-1121208',\n",
       " 'cat-1121664',\n",
       " 'cat-1122758',\n",
       " 'cat-1123000',\n",
       " 'cat-1123314',\n",
       " 'cat-1123351',\n",
       " 'cat-1123606',\n",
       " 'cat-1123628',\n",
       " 'cat-1125753',\n",
       " 'cat-114960',\n",
       " 'cat-115994',\n",
       " 'cat-116003',\n",
       " 'cat-116210',\n",
       " 'cat-116485',\n",
       " 'cat-116510',\n",
       " 'cat-116555',\n",
       " 'cat-116556',\n",
       " 'cat-116705',\n",
       " 'cat-117089',\n",
       " 'cat-119180',\n",
       " 'cat-121130',\n",
       " 'cat-1315131',\n",
       " 'cat-1324300',\n",
       " 'cat-1324836',\n",
       " 'cat-1328476',\n",
       " 'cat-1334564',\n",
       " 'cat-1338147',\n",
       " 'cat-1338908',\n",
       " 'cat-1339142',\n",
       " 'cat-1339618',\n",
       " 'cat-1539825',\n",
       " 'cat-1540398',\n",
       " 'cat-1550076',\n",
       " 'cat-1555666',\n",
       " 'cat-1559541',\n",
       " 'cat-1561642',\n",
       " 'cat-1562395',\n",
       " 'cat-1564208',\n",
       " 'cat-1570551',\n",
       " 'cat-1580153',\n",
       " 'cat-1582822',\n",
       " 'cat-1585863',\n",
       " 'cat-1590404',\n",
       " 'cat-17457',\n",
       " 'cat-17490',\n",
       " 'cat-1873568',\n",
       " 'cat-2148498',\n",
       " 'cat-2149779',\n",
       " 'cat-2149905',\n",
       " 'cat-2163782',\n",
       " 'cat-2171432',\n",
       " 'cat-2172259',\n",
       " 'cat-2179964',\n",
       " 'cat-2179983',\n",
       " 'cat-2179985',\n",
       " 'cat-2179991',\n",
       " 'cat-2180666',\n",
       " 'cat-2180770',\n",
       " 'cat-2185390',\n",
       " 'cat-2186888',\n",
       " 'cat-2188294',\n",
       " 'cat-2189046',\n",
       " 'cat-2189444',\n",
       " 'cat-2189733',\n",
       " 'cat-2190019',\n",
       " 'cat-2198828',\n",
       " 'cat-2199507',\n",
       " 'cat-2407979',\n",
       " 'cat-2407990',\n",
       " 'cat-2408311',\n",
       " 'cat-2411262',\n",
       " 'cat-2412684',\n",
       " 'cat-2415300',\n",
       " 'cat-2419044',\n",
       " 'cat-2419963',\n",
       " 'cat-2434636',\n",
       " 'cat-2438150',\n",
       " 'cat-2439150',\n",
       " 'cat-2439459',\n",
       " 'cat-2441034',\n",
       " 'cat-2441171',\n",
       " 'cat-2441965',\n",
       " 'cat-247118',\n",
       " 'cat-247359',\n",
       " 'cat-247372',\n",
       " 'cat-247955',\n",
       " 'cat-249028',\n",
       " 'cat-251946',\n",
       " 'cat-2526982',\n",
       " 'cat-256379',\n",
       " 'cat-257900',\n",
       " 'cat-258209',\n",
       " 'cat-258217',\n",
       " 'cat-259429',\n",
       " 'cat-262861',\n",
       " 'cat-263739',\n",
       " 'cat-265724',\n",
       " 'cat-267179',\n",
       " 'cat-270662',\n",
       " 'cat-270916',\n",
       " 'cat-271006',\n",
       " 'cat-271398',\n",
       " 'cat-2716276',\n",
       " 'cat-2720627',\n",
       " 'cat-2748412',\n",
       " 'cat-2751904',\n",
       " 'cat-2877827',\n",
       " 'cat-3052009',\n",
       " 'cat-3056235',\n",
       " 'cat-420580',\n",
       " 'cat-420586',\n",
       " 'cat-420618',\n",
       " 'cat-482252',\n",
       " 'cat-482303',\n",
       " 'cat-487858',\n",
       " 'cat-491530',\n",
       " 'cat-491544',\n",
       " 'cat-491560',\n",
       " 'cat-491602',\n",
       " 'cat-502014',\n",
       " 'cat-505420',\n",
       " 'cat-505611',\n",
       " 'cat-506174',\n",
       " 'cat-506332',\n",
       " 'cat-506560',\n",
       " 'cat-655183',\n",
       " 'cat-658797',\n",
       " 'cat-659075',\n",
       " 'cat-661279',\n",
       " 'cat-787253',\n",
       " 'cat-788423',\n",
       " 'cat-788433',\n",
       " 'cat-789787',\n",
       " 'cat-789788',\n",
       " 'cat-791355',\n",
       " 'cat-791519',\n",
       " 'cat-792075',\n",
       " 'cat-792341',\n",
       " 'cat-794072',\n",
       " 'cat-794236',\n",
       " 'cat-795702',\n",
       " 'cat-796025',\n",
       " 'cat-798139',\n",
       " 'cat-804994',\n",
       " 'cat-805079',\n",
       " 'cat-805158',\n",
       " 'cat-805223',\n",
       " 'cat-805795',\n",
       " 'cat-805799',\n",
       " 'cat-805806',\n",
       " 'cat-807627',\n",
       " 'cat-807656',\n",
       " 'cat-807661',\n",
       " 'cat-809769',\n",
       " 'cat-811597',\n",
       " 'cat-811610',\n",
       " 'cat-811620',\n",
       " 'cat-811621',\n",
       " 'cat-813331',\n",
       " 'cat-813388',\n",
       " 'cat-813434',\n",
       " 'cat-813886',\n",
       " 'cat-813937',\n",
       " 'cat-814543',\n",
       " 'cat-814754',\n",
       " 'cat-814794',\n",
       " 'cat-817013',\n",
       " 'cat-821251',\n",
       " 'cat-822473',\n",
       " 'cat-823812',\n",
       " 'cat-825925',\n",
       " 'cat-826305',\n",
       " 'cat-826367',\n",
       " 'cat-826868',\n",
       " 'cat-827255',\n",
       " 'cat-827260',\n",
       " 'cat-829282',\n",
       " 'cat-830761',\n",
       " 'cat-831306',\n",
       " 'cat-831331',\n",
       " 'cat-832646',\n",
       " 'cat-832650',\n",
       " 'cat-832651',\n",
       " 'cat-832677',\n",
       " 'cat-836391',\n",
       " 'cat-836437',\n",
       " 'cat-837083',\n",
       " 'cat-91390',\n",
       " 'cat-91769',\n",
       " 'cat-91771',\n",
       " 'cat-92138',\n",
       " 'cat-93002',\n",
       " 'cat-93071',\n",
       " 'cat-94417',\n",
       " 'cat-94429',\n",
       " 'cat-94440',\n",
       " 'cat-94536',\n",
       " 'cat-94772',\n",
       " 'cat-95082',\n",
       " 'cat-95097',\n",
       " 'cat-95120',\n",
       " 'cat-96522',\n",
       " 'cat-99536']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_unique_catchments(\"../data/CONUS/Filtered_LSR_withNexgenCatchmentsgt1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e44dc-e63e-4b8b-a9ee-1902914982ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
